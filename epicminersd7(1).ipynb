{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "94dba0bc-8c77-4295-85f8-4955e606ef05",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;32mDone!\n"
     ]
    }
   ],
   "source": [
    "#1 step installing requirements and repo\n",
    "print('\\033[1;32mInstalling requirements...')\n",
    "\n",
    "from IPython.display import clear_output\n",
    "!conda install -c conda-forge libglib -y & conda update -n base conda -y\n",
    "\n",
    "!pip install tqdm\n",
    "\n",
    "clear_output()\n",
    "print('\\033[1;32mDone! Conda installed!')\n",
    "\n",
    "# Clone the Stable Diffusion Web UI repository\n",
    "!git clone https://github.com/AUTOMATIC1111/stable-diffusion-webui.git\n",
    "%cd stable-diffusion-webui\n",
    "\n",
    "# Uncomment this line if you need to install the requirements\n",
    "%pip install -r requirements.txt --quiet\n",
    "\n",
    "# ControlNet extension\n",
    "#reactor extenshion\n",
    "%cd extensions/\n",
    "!git clone https://github.com/epic-miner/sd-webui-reactor.git\n",
    "!git clone https://github.com/Mikubill/sd-webui-controlnet.git\n",
    "%cd ..\n",
    "\n",
    "clear_output()\n",
    "print('\\033[1;32mDone!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29bbd758-ffba-441e-800b-6eb0e89dbddd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyngrok in /opt/conda/envs/sagemaker-distribution/lib/python3.10/site-packages (7.1.6)\n",
      "Requirement already satisfied: PyYAML>=5.1 in /opt/conda/envs/sagemaker-distribution/lib/python3.10/site-packages (from pyngrok) (6.0.1)\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter your ngrok authentication token:  2gmWwCjlXWwRAbNIZL9k3VVO4JL_2FmGxirjViwHS6MBv7j9i\n",
      "Select device (1 for CPU, 2 for GPU):  2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting ngrok tunnel: NgrokTunnel: \"https://1f50-3-20-229-229.ngrok-free.app\" -> \"http://localhost:7860\"\n",
      "Python 3.10.14 | packaged by conda-forge | (main, Mar 20 2024, 12:45:18) [GCC 12.3.0]\n",
      "Version: v1.9.3\n",
      "Commit hash: 1c0a0c4c26f78c32095ebc7f8af82f5c04fca8c0\n",
      "CUDA 11.2\n",
      "Launching Web UI with arguments: --port 7860 --skip-torch-cuda-test\n",
      "no module 'xformers'. Processing without...\n",
      "no module 'xformers'. Processing without...\n",
      "No module 'xformers'. Proceeding without it.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==============================================================================\n",
      "You are running torch 2.0.0.post200.\n",
      "The program is tested to work with torch 2.1.2.\n",
      "To reinstall the desired version, run with commandline flag --reinstall-torch.\n",
      "Beware that this will cause a lot of large files to be downloaded, as well as\n",
      "there are reports of issues with training tab on the latest version.\n",
      "\n",
      "Use --skip-version-check commandline argument to disable this check.\n",
      "==============================================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ControlNet preprocessor location: /home/studio-lab-user/stable-diffusion-webui/extensions/sd-webui-controlnet/annotator/downloads\n",
      "2024-05-25 06:10:15,584 - ControlNet - \u001b[0;32mINFO\u001b[0m - ControlNet v1.1.449\n",
      "06:10:16 - ReActor - \u001b[38;5;173mSTATUS\u001b[0m - Running v0.7.0-b7 on Device: CUDA\n",
      "Loading weights [6ce0161689] from /home/studio-lab-user/stable-diffusion-webui/models/Stable-diffusion/v1-5-pruned-emaonly.safetensors\n",
      "2024-05-25 06:10:16,390 - ControlNet - \u001b[0;32mINFO\u001b[0m - ControlNet UI callback registered.\n",
      "Running on local URL:  http://127.0.0.1:7860\n",
      "Creating model from config: /home/studio-lab-user/stable-diffusion-webui/configs/v1-inference.yaml\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n",
      "Startup time: 16.7s (prepare environment: 4.1s, import torch: 7.0s, import gradio: 1.0s, setup paths: 0.5s, initialize shared: 0.3s, other imports: 0.7s, load scripts: 1.7s, create ui: 0.6s, gradio launch: 0.7s).\n",
      "Applying attention optimization: Doggettx... done.\n",
      "Model loaded in 4.4s (load weights from disk: 1.0s, create model: 0.5s, apply weights to model: 2.6s, calculate empty prompt: 0.1s).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total progress:   0%|          | 0/20 [00:00<?, ?it/s]\u001b[A\n",
      "Total progress:  10%|█         | 2/20 [00:00<00:01, 10.64it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 4/20 [00:01<00:03,  4.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total progress:  20%|██        | 4/20 [00:00<00:02,  6.84it/s]\u001b[A\n",
      "Total progress:  25%|██▌       | 5/20 [00:00<00:02,  6.38it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 6/20 [00:01<00:02,  4.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total progress:  30%|███       | 6/20 [00:00<00:02,  6.08it/s]\u001b[A\n",
      "Total progress:  35%|███▌      | 7/20 [00:01<00:02,  5.84it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 8/20 [00:01<00:02,  5.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total progress:  40%|████      | 8/20 [00:01<00:02,  5.72it/s]\u001b[A\n",
      "Total progress:  45%|████▌     | 9/20 [00:01<00:01,  5.62it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 10/20 [00:02<00:01,  5.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total progress:  50%|█████     | 10/20 [00:01<00:01,  5.54it/s]\u001b[A\n",
      "Total progress:  55%|█████▌    | 11/20 [00:01<00:01,  5.51it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 12/20 [00:02<00:01,  5.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total progress:  60%|██████    | 12/20 [00:02<00:01,  5.49it/s]\u001b[A\n",
      "Total progress:  65%|██████▌   | 13/20 [00:02<00:01,  5.48it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 14/20 [00:02<00:01,  5.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total progress:  70%|███████   | 14/20 [00:02<00:01,  5.50it/s]\u001b[A\n",
      "Total progress:  75%|███████▌  | 15/20 [00:02<00:00,  5.51it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 16/20 [00:03<00:00,  5.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total progress:  80%|████████  | 16/20 [00:02<00:00,  5.45it/s]\u001b[A\n",
      "Total progress:  85%|████████▌ | 17/20 [00:02<00:00,  5.45it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 18/20 [00:03<00:00,  5.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total progress:  90%|█████████ | 18/20 [00:03<00:00,  5.46it/s]\u001b[A\n",
      "Total progress:  95%|█████████▌| 19/20 [00:03<00:00,  5.47it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:04<00:00,  4.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total progress: 100%|██████████| 20/20 [00:03<00:00,  5.48it/s]\u001b[A\n",
      "Total progress: 100%|██████████| 20/20 [00:03<00:00,  5.18it/s]\u001b[A\n"
     ]
    }
   ],
   "source": [
    "# Install ngrok\n",
    "!pip install pyngrok\n",
    "import os\n",
    "from pyngrok import ngrok\n",
    "import subprocess\n",
    "import threading\n",
    "\n",
    "# Prompt user for ngrok authentication token\n",
    "ngrok_auth_token = input(\"Enter your ngrok authentication token: \")\n",
    "ngrok.set_auth_token(ngrok_auth_token)\n",
    "\n",
    "# Function to start the Web UI\n",
    "def start_webui(device=\"cpu\"):\n",
    "    os.chdir(\"/home/studio-lab-user/stable-diffusion-webui\")\n",
    "    if device == \"cpu\":\n",
    "        subprocess.run([\"python\", \"launch.py\", \"--port\", \"7860\", \"--skip-torch-cuda-test\"])\n",
    "    elif device == \"gpu\":\n",
    "        subprocess.run([\"python\", \"launch.py\", \"--port\", \"7860\"])\n",
    "\n",
    "# Function to start ngrok\n",
    "def start_ngrok():\n",
    "    public_url = ngrok.connect(7860)\n",
    "    print(f'Starting ngrok tunnel: {public_url}')\n",
    "\n",
    "# Start the Web UI in a separate thread\n",
    "webui_thread = threading.Thread(target=start_webui)\n",
    "\n",
    "# Prompt user for device selection\n",
    "device_option = input(\"Select device (1 for CPU, 2 for GPU): \")\n",
    "if device_option == \"1\":\n",
    "    webui_thread.start()\n",
    "elif device_option == \"2\":\n",
    "    webui_thread.start()\n",
    "    # Start ngrok for GPU\n",
    "    start_ngrok()\n",
    "else:\n",
    "    print(\"Invalid option selected.\")\n",
    "\n",
    "# Keep the script running to maintain the ngrok tunnel\n",
    "webui_thread.join()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f75479c4-5ed2-4a23-9440-d966679175f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#check storage\n",
    "\n",
    "import shutil\n",
    "\n",
    "total, used, free = shutil.disk_usage(\"/\")\n",
    "\n",
    "print(\"Total: %d GB\" % (total // (2**30)))\n",
    "print(\"Used: %d GB\" % (used // (2**30)))\n",
    "print(\"Free: %d GB\" % (free // (2**30)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23ada4dc-1d90-46de-bd64-713eb2a468bd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#chek avalable storage and delete file by selecting numbrs\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "import subprocess\n",
    "\n",
    "def check_disk_usage():\n",
    "    \"\"\"Check the current disk usage.\"\"\"\n",
    "    total, used, free = shutil.disk_usage(\"/\")\n",
    "    return free // (2**30)  # Free space in GB\n",
    "\n",
    "def get_largest_files_and_dirs(path=\"/\", n=10):\n",
    "    \"\"\"Get the largest files and directories in the specified path.\"\"\"\n",
    "    result = subprocess.run(\n",
    "        [\"du\", \"-ah\", path], stdout=subprocess.PIPE, text=True\n",
    "    )\n",
    "    lines = result.stdout.splitlines()\n",
    "    sizes = []\n",
    "    for line in lines:\n",
    "        size, name = line.split(maxsplit=1)\n",
    "        if size.endswith('G'):\n",
    "            size = float(size[:-1]) * 1024 ** 3\n",
    "        elif size.endswith('M'):\n",
    "            size = float(size[:-1]) * 1024 ** 2\n",
    "        elif size.endswith('K'):\n",
    "            size = float(size[:-1]) * 1024\n",
    "        else:\n",
    "            size = float(size)\n",
    "        sizes.append((size, name))\n",
    "    sizes.sort(reverse=True, key=lambda x: x[0])\n",
    "    return sizes[:n]\n",
    "\n",
    "def delete_file(path):\n",
    "    \"\"\"Delete the specified file or directory.\"\"\"\n",
    "    if os.path.isdir(path):\n",
    "        shutil.rmtree(path)\n",
    "    else:\n",
    "        os.remove(path)\n",
    "\n",
    "def main():\n",
    "    print(\"Checking current disk usage...\")\n",
    "    free_space = check_disk_usage()\n",
    "    if free_space > 5:\n",
    "        print(\"You have enough free space.\")\n",
    "        return\n",
    "\n",
    "    print(\"\\nListing the largest files and directories...\")\n",
    "    largest_items = get_largest_files_and_dirs()\n",
    "    for i, (size, name) in enumerate(largest_items, start=1):\n",
    "        print(f\"{i}. {size / (1024 ** 2):.2f} MB - {name}\")\n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            choice = int(input(\"\\nEnter the number of the file/directory to delete (or 0 to exit): \"))\n",
    "            if choice == 0:\n",
    "                break\n",
    "            if choice < 1 or choice > len(largest_items):\n",
    "                print(\"Invalid choice. Please enter a number from the list.\")\n",
    "                continue\n",
    "            _, path = largest_items[choice - 1]\n",
    "            delete_file(path)\n",
    "            print(f\"Deleted {path}.\")\n",
    "            free_space = check_disk_usage()\n",
    "            print(f\"Free space after deletion: {free_space} GB\")\n",
    "            if free_space > 5:\n",
    "                print(\"You now have enough free space.\")\n",
    "                break\n",
    "        except ValueError:\n",
    "            print(\"Invalid input. Please enter a number.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02815548-1ab2-49cb-a5db-b8f1604b948b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sagemaker-distribution:Python",
   "language": "python",
   "name": "conda-env-sagemaker-distribution-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
